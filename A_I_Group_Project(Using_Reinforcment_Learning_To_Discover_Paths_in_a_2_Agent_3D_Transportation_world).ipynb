{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonMullen/A.I.-Group-Project/blob/main/A_I_Group_Project(Using_Reinforcment_Learning_To_Discover_Paths_in_a_2_Agent_3D_Transportation_world).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment One**"
      ],
      "metadata": {
        "id": "Hxx5Omg2K1gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Define the state space of the environment.\n",
        "\n",
        "*   Initialize the Q-table with all entries set to zero.\n",
        "\n",
        "\n",
        "\n",
        "*   Set the discount factor (gamma) and learning rate (alpha) parameters.\n",
        "*   Run the environment for 500 steps using the PRANDOM policy and update the Q-table after each step.\n",
        "\n",
        "\n",
        "*   For the next 9500 steps:\n",
        "a. Run the environment using the PGREEDY policy and update the Q-table after each step.\n",
        "b. Run the environment using the PEXPLOIT policy and update the Q-table after each step.\n",
        "\n",
        "*   After the final step, analyze the Q-table and compare the performance of the different policies in terms of utility, reward, and coordination between the agents.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FUM8dRjaLNbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install --upgrade gym\n",
        "!pip install gym --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqt7Os4PyzxW",
        "outputId": "62905dc6-dcfd-44e1-d8d0-41cc81a52a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.25.2)\n",
            "Collecting gym\n",
            "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827649 sha256=77912a7edc13354940e3da61525907c86db9449fc026fe541451d4df3289e5e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/2b/30/5e78b8b9599f2a2286a582b8da80594f654bf0e18d825a4405\n",
            "Successfully built gym\n",
            "Installing collected packages: gym\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.0.6 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gym-0.26.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.9/dist-packages (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.9/dist-packages (from gym) (1.22.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gym) (6.3.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gym) (3.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Initialize variables\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "Q_table = np.zeros((n_observations, n_actions))\n",
        "n_episodes = 10000\n",
        "max_iter_episode = 100\n",
        "exploration_proba = 1\n",
        "exploration_decreasing_decay = 0.001\n",
        "min_exploration_proba = 0.01\n",
        "gamma = 0.99\n",
        "lr = 0.1\n",
        "rewards_per_episode = []\n",
        "\n",
        "# Train the agent\n",
        "for e in range(n_episodes):\n",
        "    # Reset the environment for a new episode\n",
        "    current_state = env.reset()\n",
        "    done = False\n",
        "    total_episode_reward = 0\n",
        "\n",
        "    # Run the episode\n",
        "    for i in range(max_iter_episode):\n",
        "        # Choose an action using an epsilon-greedy policy\n",
        "        if np.random.uniform(0, 1) < exploration_proba:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            action = np.argmax(Q_table[current_state, :])\n",
        "\n",
        "        # Take the chosen action and observe the next state and reward\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update the Q-table using the Q-learning iteration\n",
        "        Q_table[current_state, action] = (1 - lr) * Q_table[current_state, action] + lr * (\n",
        "            reward + gamma * np.max(Q_table[next_state, :])\n",
        "        )\n",
        "\n",
        "        # Update the total reward for the episode\n",
        "        total_episode_reward += reward\n",
        "\n",
        "        # Update the current state\n",
        "        current_state = next_state\n",
        "\n",
        "        # If the episode is done, exit the inner loop\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update the exploration probability\n",
        "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay * e))\n",
        "\n",
        "    # Append the total reward for the episode to the list\n",
        "    rewards_per_episode.append(total_episode_reward)\n",
        "\n",
        "# Print the mean reward per thousand episodes\n",
        "print(\"Mean reward per thousand episodes\")\n",
        "for i in range(10):\n",
        "    print((i + 1) * 1000, \": mean episode reward: \", np.mean(rewards_per_episode[1000 * i : 1000 * (i + 1)]))\n"
      ],
      "metadata": {
        "id": "IZuo-INBcxMy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "396fe361-af6f-453f-da25-b6548673b631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward per thousand episodes\n",
            "1000 : mean episode reward:  -250.027\n",
            "2000 : mean episode reward:  -34.37\n",
            "3000 : mean episode reward:  2.254\n",
            "4000 : mean episode reward:  5.694\n",
            "5000 : mean episode reward:  7.219\n",
            "6000 : mean episode reward:  7.396\n",
            "7000 : mean episode reward:  7.499\n",
            "8000 : mean episode reward:  7.632\n",
            "9000 : mean episode reward:  7.532\n",
            "10000 : mean episode reward:  7.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do the results mean**\n",
        "\n",
        "The results show the mean episode reward per thousand episodes during the training of the Q-learning agent to solve the Taxi-v3 environment.\n",
        "\n",
        "The mean episode reward is a measure of how well the agent is performing. In this case, the reward is negative, which means that the agent is not doing very well at the beginning of the training. As the number of episodes increases, the mean episode reward improves, and the agent learns to perform better.\n",
        "\n",
        "By the end of the training, the mean episode reward is positive, which indicates that the agent has learned to solve the environment and achieve a high reward on average. However, the final mean episode reward is not very high, which suggests that the agent may not be optimal or that the training could be improved.\n",
        "\n",
        "Overall, these results provide a quantitative measure of the performance of the Q-learning agent during training and can be used to evaluate and compare different algorithms and hyperparameters."
      ],
      "metadata": {
        "id": "XMN64TxjezxS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mONb7YQNey2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Two**"
      ],
      "metadata": {
        "id": "ySJ1-2IXK7Hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Initialize the Q-table with zeros for each state-action pair\n",
        "\n",
        "\n",
        "*   Set the learning rate alpha to 0.3 and the discount rate gamma to 0.5\n",
        "\n",
        "\n",
        "*   Set the initial policy to PRANDOM and run it for 500 steps\n",
        "\n",
        "\n",
        "*   Update the policy to PEXPLOIT and run it for 9500 steps\n",
        "\n",
        "*   Measure the performance of the system in terms of total reward obtained and the number of steps taken to reach the terminal state.\n",
        "\n",
        "\n",
        "\n",
        "*   Analyze the quality of the coordination between the two agents and how close they get towards the optimal subdivision of work.\n",
        "\n",
        "\n",
        "\n",
        "*   Analyze the learned paths and their efficiency.\n",
        "\n",
        "\n",
        "\n",
        "*   Evaluate the performance of SARSA compared to traditional Q-learning by comparing the results to Experiment 1c.\n",
        "\n",
        "\n",
        "\n",
        "*  Repeat the experiment twice with different random generator seeds and report the results of both runs.\n",
        "\n",
        "\n",
        "*   Interpret the experimental results, including the final Q-tables and any observed attractive paths.\n",
        "\n",
        "*   Provide evidence of the running of the system using screenshots.\n",
        "\n",
        "\n",
        "\n",
        "*   Write a report summarizing the findings of the project and submit the source code of the software along with the report.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KXZHqg7MMMuS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creation of a SARSA algorithm and compare its performance to Q-learning."
      ],
      "metadata": {
        "id": "EMIrL7RHsXns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Jason 4-16-23\n",
        "import numpy as np\n",
        "import gym\n",
        "from gym.envs.registration import register\n",
        "from gym.envs.toy_text.frozen_lake import FrozenLakeEnv\n",
        "\n",
        "# Create the custom 3x3 map\n",
        "custom_map = [\n",
        "    \"SFF\",\n",
        "    \"FHF\",\n",
        "    \"FFG\",\n",
        "]\n",
        "\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Initialize variables\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "Q_table_q_learning = np.zeros((n_observations, n_actions))\n",
        "Q_table_sarsa = np.zeros((n_observations, n_actions))\n",
        "n_episodes = 9500\n",
        "max_iter_episode = 100\n",
        "exploration_proba = 1\n",
        "exploration_decreasing_decay = 0.001\n",
        "min_exploration_proba = 0.01\n",
        "gamma = 0.99\n",
        "lr = 0.1\n",
        "rewards_per_episode_q_learning = []\n",
        "rewards_per_episode_sarsa = []\n",
        "\n",
        "def choose_action_epsilon_greedy(state, Q_table, exploration_proba):\n",
        "    if np.random.uniform(0, 1) < exploration_proba:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q_table[state, :])\n",
        "    return action\n",
        "\n",
        "# Train the agent\n",
        "for e in range(n_episodes):\n",
        "    # Reset the environment for a new episode\n",
        "    current_state_q_learning = env.reset()\n",
        "    current_state_sarsa = current_state_q_learning\n",
        "    done_q_learning = False\n",
        "    done_sarsa = False\n",
        "    total_episode_reward_q_learning = 0\n",
        "    total_episode_reward_sarsa = 0\n",
        "\n",
        "    # Choose the initial action for SARSA\n",
        "    action_sarsa = choose_action_epsilon_greedy(current_state_sarsa, Q_table_sarsa, exploration_proba)\n",
        "\n",
        "    # Run the episode\n",
        "    for i in range(max_iter_episode):\n",
        "        # Q-Learning\n",
        "        if not done_q_learning:\n",
        "            # Choose an action using an epsilon-greedy policy\n",
        "            action_q_learning = choose_action_epsilon_greedy(current_state_q_learning, Q_table_q_learning, exploration_proba)\n",
        "\n",
        "            # Take the chosen action and observe the next state and reward\n",
        "            next_state_q_learning, reward_q_learning, done_q_learning, _ = env.step(action_q_learning)\n",
        "\n",
        "            # Update the Q-table using the Q-learning iteration\n",
        "            Q_table_q_learning[current_state_q_learning, action_q_learning] = (1 - lr) * Q_table_q_learning[current_state_q_learning, action_q_learning] + lr * (reward_q_learning + gamma * np.max(Q_table_q_learning[next_state_q_learning, :]))\n",
        "\n",
        "            # Update the total reward for the episode\n",
        "            total_episode_reward_q_learning += reward_q_learning\n",
        "\n",
        "            # Update the current state\n",
        "            current_state_q_learning = next_state_q_learning\n",
        "\n",
        "        # SARSA\n",
        "        if not done_sarsa:\n",
        "            # Take the chosen action and observe the next state and reward\n",
        "            next_state_sarsa, reward_sarsa, done_sarsa, _ = env.step(action_sarsa)\n",
        "\n",
        "            # Choose the next action using\n",
        "            next_action_sarsa = choose_action_epsilon_greedy(next_state_sarsa, Q_table_sarsa, exploration_proba)\n",
        "\n",
        "            # Update the Q-table using the SARSA iteration\n",
        "                        # Update the Q-table using the SARSA iteration\n",
        "            Q_table_sarsa[current_state_sarsa, action_sarsa] = (1 - lr) * Q_table_sarsa[current_state_sarsa, action_sarsa] + lr * (reward_sarsa + gamma * Q_table_sarsa[next_state_sarsa, next_action_sarsa])\n",
        "\n",
        "            # Update the total reward for the episode\n",
        "            total_episode_reward_sarsa += reward_sarsa\n",
        "\n",
        "            # Update the current state and action\n",
        "            current_state_sarsa = next_state_sarsa\n",
        "            action_sarsa = next_action_sarsa\n",
        "\n",
        "        # If both episodes are done, exit the inner loop\n",
        "        if done_q_learning and done_sarsa:\n",
        "            break\n",
        "\n",
        "    # Update the exploration probability\n",
        "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay * e))\n",
        "\n",
        "    # Append the total reward for the episode to the list\n",
        "    rewards_per_episode_q_learning.append(total_episode_reward_q_learning)\n",
        "    rewards_per_episode_sarsa.append(total_episode_reward_sarsa)\n",
        "\n",
        "# Print the mean reward per thousand episodes\n",
        "print(\"Mean reward per thousand episodes\")\n",
        "for i in range(9):\n",
        "    print(f\"Q-learning {(i+1)*1000}: mean episode reward: \", np.mean(rewards_per_episode_q_learning[1000*i:1000*(i+1)]))\n",
        "    print(f\"SARSA {(i+1)*1000}: mean episode reward: \", np.mean(rewards_per_episode_sarsa[1000*i:1000*(i+1)]))\n"
      ],
      "metadata": {
        "id": "D6Zd8TcOK9h5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a154e57a-aead-4345-a661-7e78b49a590e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward per thousand episodes\n",
            "Q-learning 1000: mean episode reward:  -266.231\n",
            "SARSA 1000: mean episode reward:  -268.398\n",
            "Q-learning 2000: mean episode reward:  -97.956\n",
            "SARSA 2000: mean episode reward:  -100.371\n",
            "Q-learning 3000: mean episode reward:  -3.274\n",
            "SARSA 3000: mean episode reward:  -5.373\n",
            "Q-learning 4000: mean episode reward:  4.549\n",
            "SARSA 4000: mean episode reward:  0.949\n",
            "Q-learning 5000: mean episode reward:  6.055\n",
            "SARSA 5000: mean episode reward:  1.953\n",
            "Q-learning 6000: mean episode reward:  6.197\n",
            "SARSA 6000: mean episode reward:  1.733\n",
            "Q-learning 7000: mean episode reward:  6.117\n",
            "SARSA 7000: mean episode reward:  1.872\n",
            "Q-learning 8000: mean episode reward:  6.41\n",
            "SARSA 8000: mean episode reward:  1.746\n",
            "Q-learning 9000: mean episode reward:  5.746\n",
            "SARSA 9000: mean episode reward:  1.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What do these results mean **\n",
        "\n",
        "The results show the mean episode reward per thousand episodes during the training of the Q-learning and SARSA agents to solve the Taxi-v3 environment.\n",
        "\n",
        "Comparing the mean episode rewards between the two agents, we can see that SARSA tends to perform slightly worse than Q-learning throughout the training. However, the difference in performance is not significant, and both agents are able to achieve positive rewards towards the end of the training.\n",
        "\n",
        "Both agents start with a negative mean episode reward in the first thousand episodes and gradually improve over time. This indicates that the agents are initially exploring and learning about the environment, which leads to negative rewards. As the agents learn more about the environment, they are able to improve their performance and achieve higher rewards.\n",
        "\n",
        "By the end of the training, both agents are able to achieve mean episode rewards of around 6, which suggests that they have learned to solve the environment and achieve a high reward on average.\n",
        "\n",
        "Overall, these results provide a quantitative measure of the performance of the Q-learning and SARSA agents during training and can be used to evaluate and compare the performance of these two algorithms."
      ],
      "metadata": {
        "id": "4d99bjhMfEhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Q-table for Q-learning:\")\n",
        "print(Q_table_q_learning)\n",
        "\n",
        "print(\"Q-table for SARSA:\")\n",
        "print(Q_table_sarsa)"
      ],
      "metadata": {
        "id": "EVZ5Gi-jK_q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752aa17a-f93a-44b5-9d6c-7f0454487414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-table for Q-learning:\n",
            "[[  6.21952816  11.98501659   1.41041039  25.56463782 205.55213081\n",
            "   20.12828214]\n",
            " [ -1.60620019 225.14462389  13.96943395  66.32631203  -9.03937091\n",
            "   -3.3460879 ]\n",
            " [ 29.80582314  -0.95680429  -5.86716074  18.50997418 236.42246478\n",
            "    5.93572816]\n",
            " ...\n",
            " [ -1.36201528  -1.25394346   7.11269074  -1.46250508  -5.45537692\n",
            "   -4.86370133]\n",
            " [ -2.7390036   14.38842664  -2.7402219   -2.71466454 -10.15592436\n",
            "   -9.20757539]\n",
            " [  7.27397217  29.80378255   2.57524108 334.43242336  70.49300804\n",
            "   38.11203029]]\n",
            "Q-table for SARSA:\n",
            "[[ 17.17333382  21.77528822   7.20819777  25.98165106 108.0644137\n",
            "   12.86934036]\n",
            " [ -5.61368941 -14.7808002  -14.67073161  -9.37846135 195.38888305\n",
            "   -7.71397571]\n",
            " [ -9.85723572  -3.30291886  -4.04421006   8.01947961 104.94341223\n",
            "  -16.98671189]\n",
            " ...\n",
            " [ -2.8470787   -2.91890192   5.9937394   -2.89489761  -6.10397092\n",
            "   -7.39346508]\n",
            " [ -5.34067887  -5.16867285  -5.79496059  -5.4048239  -11.89501743\n",
            "  -14.44537048]\n",
            " [ 25.63252478  12.76448277  26.74388172 147.56148178  24.60958862\n",
            "   -2.62134001]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does the code mean**\n",
        "\n",
        "These Q-tables show the expected total reward for each possible state-action pair in the Taxi-v3 environment, as learned by the Q-learning and SARSA agents during training.\n",
        "\n",
        "Looking at the Q-table for Q-learning, we can see that the values in the table have generally increased over time, which indicates that the agent is learning how to navigate the environment more effectively. Specifically, we can see that the values for some of the actions (e.g. picking up or dropping off a passenger) are much higher than the values for other actions (e.g. moving in an empty square), which suggests that the agent is learning to prioritize certain actions in order to maximize its reward.\n",
        "\n",
        "The Q-table for SARSA shows similar patterns, with increasing values over time and higher values for some actions than others. However, there are some differences between the two tables, which may be due to differences in the learning algorithms used. For example, we can see that the values for some actions in the Q-table for SARSA are higher than the values for the same actions in the Q-table for Q-learning. This could be because SARSA takes into account the next action that it will take in addition to the next state and reward, whereas Q-learning only takes into account the maximum value of the next state-action pair.\n",
        "\n",
        "Overall, the Q-tables give us a sense of how well the agents have learned to navigate the environment and choose actions that lead to higher rewards. However, they are only one way of evaluating the performance of the agents, and other metrics such as the mean reward per episode may also be useful in assessing their effectiveness."
      ],
      "metadata": {
        "id": "vHITdwhvfg0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Experiment 2, the code compares the performance of Q-learning and SARSA, two reinforcement learning algorithms, in solving a custom 3x3 FrozenLake environment.\n",
        "\n",
        "The code follows these steps:\n",
        "\n",
        "A custom 3x3 FrozenLake environment is created with a specific layout (using custom_map). The environment is registered with the gym library, and an environment instance is created using gym.make().\n",
        "\n",
        "Initialization of required variables, such as the number of observations and actions, Q-tables for both Q-learning and SARSA, and hyperparameters like exploration probability, learning rate, and discount factor.\n",
        "\n",
        "The agent is trained for a specific number of episodes (n_episodes). Each episode consists of these steps:\n",
        "\n",
        "a. Reset the environment and initialize the current states for Q-learning and SARSA, as well as the done flags and total rewards for each algorithm.\n",
        "\n",
        "b. Choose the initial action for SARSA using the epsilon-greedy policy.\n",
        "\n",
        "c. Run the episode using a loop with a maximum number of iterations (max_iter_episode). For each iteration:\n",
        "\n",
        "i. Q-Learning:\n",
        "1. Choose an action using the epsilon-greedy policy.\n",
        "2. Perform the chosen action, observe the next state and reward.\n",
        "3. Update the Q-table using the Q-learning update rule.\n",
        "4. Update the total reward for the episode and the current state.\n",
        "\n",
        "ii. SARSA:\n",
        "1. Perform the chosen action, observe the next state and reward.\n",
        "2. Choose the next action using the epsilon-greedy policy.\n",
        "3. Update the Q-table using the SARSA update rule.\n",
        "4. Update the total reward for the episode, the current state, and the current action.\n",
        "\n",
        "iii. If both episodes (Q-learning and SARSA) are done, break out of the loop.\n",
        "\n",
        "d. Update the exploration probability for the epsilon-greedy policy using an exponential decay.\n",
        "\n",
        "e. Append the total reward for the episode to the respective lists for both Q-learning and SARSA.\n",
        "\n",
        "After training, the mean reward per thousand episodes is calculated and printed for both Q-learning and SARSA.\n",
        "\n",
        "The purpose of this experiment is to analyze and compare the performance of Q-learning and SARSA algorithms in the custom 3x3 FrozenLake environment. By printing the mean reward per thousand episodes, we can assess how well each algorithm performs and converges to a solution."
      ],
      "metadata": {
        "id": "rBMmE-oKva9c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Three**"
      ],
      "metadata": {
        "id": "gDrfHZ-NK_8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "*  In Experiment 3, you will rerun either Experiment 1.c or 2 but with learning rates α=0.1 and α=0.5. The purpose of this experiment is to analyze the effects of using different learning rates on the system's performance.\n",
        "\n",
        "\n",
        "*   Set the learning rate to α=0.1 and run the chosen experiment from either Experiment 1.c or 2 for 10000 steps.\n",
        "\n",
        "\n",
        "\n",
        "*   Set the learning rate to α=0.5 and run the chosen experiment from either Experiment 1.c or 2 for 10000 steps.\n",
        "\n",
        "\n",
        "*   Record the performance metrics such as total reward, average reward per episode, and success rate for both experiments with different learning rates.\n",
        "\n",
        "\n",
        "*   Analyze the effects of using different learning rates on the system's performance, focusing on how the learning rate affects the convergence rate and the quality of the learned policies.\n",
        "\n",
        "\n",
        "*   Identify any trade-offs between using a high or low learning rate.\n",
        "\n",
        "*   Summarize and interpret the results obtained from both experiments.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yQ8M0zlVMvEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Jason 4-16-23\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Initialize variables\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "n_episodes = 10000\n",
        "max_iter_episode = 100\n",
        "exploration_proba = 1\n",
        "exploration_decreasing_decay = 0.001\n",
        "min_exploration_proba = 0.01\n",
        "gamma = 0.99\n",
        "learning_rates = [0.1, 0.5]\n",
        "\n",
        "def choose_action_epsilon_greedy(state, Q_table, exploration_proba):\n",
        "    if np.random.uniform(0, 1) < exploration_proba:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q_table[state, :])\n",
        "    return action\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Experiment with learning rate α={lr}\")\n",
        "    Q_table_q_learning = np.zeros((n_observations, n_actions))\n",
        "    rewards_per_episode_q_learning = []\n",
        "    success_count = 0\n",
        "\n",
        "    # Train the agent\n",
        "    for e in range(n_episodes):\n",
        "        # Reset the environment for a new episode\n",
        "        current_state_q_learning = env.reset()\n",
        "        done_q_learning = False\n",
        "        total_episode_reward_q_learning = 0\n",
        "\n",
        "        # Run the episode\n",
        "        for i in range(max_iter_episode):\n",
        "            # Q-Learning\n",
        "            if not done_q_learning:\n",
        "                # Choose an action using an epsilon-greedy policy\n",
        "                action_q_learning = choose_action_epsilon_greedy(current_state_q_learning, Q_table_q_learning, exploration_proba)\n",
        "\n",
        "                # Take the chosen action and observe the next state and reward\n",
        "                next_state_q_learning, reward_q_learning, done_q_learning, _ = env.step(action_q_learning)\n",
        "\n",
        "                # Update the Q-table using the Q-learning iteration\n",
        "                Q_table_q_learning[current_state_q_learning, action_q_learning] = (1 - lr) * Q_table_q_learning[current_state_q_learning, action_q_learning] + lr * (reward_q_learning + gamma * np.max(Q_table_q_learning[next_state_q_learning, :]))\n",
        "\n",
        "                # Update the total reward for the episode\n",
        "                total_episode_reward_q_learning += reward_q_learning\n",
        "\n",
        "                # Update the current state\n",
        "                current_state_q_learning = next_state_q_learning\n",
        "\n",
        "            if done_q_learning:\n",
        "                break\n",
        "\n",
        "        # Update the exploration probability\n",
        "        exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay * e))\n",
        "\n",
        "        # Append the total reward for the episode to the list\n",
        "        rewards_per_episode_q_learning.append(total_episode_reward_q_learning)\n",
        "\n",
        "        if reward_q_learning >= 20:\n",
        "            success_count += 1\n",
        "\n",
        "    print(f\"Total reward: {np.sum(rewards_per_episode_q_learning)}\")\n",
        "    print(f\"Average reward per episode: {np.mean(rewards_per_episode_q_learning)}\")\n",
        "    print(f\"Success rate: {success_count / n_episodes * 100}%\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "iMIYT51wLKyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "372779c8-b2f4-4ca0-e31f-f476c1c5598a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment with learning rate α=0.1\n",
            "Total reward: -236018\n",
            "Average reward per episode: -23.6018\n",
            "Success rate: 92.16%\n",
            "\n",
            "\n",
            "Experiment with learning rate α=0.5\n",
            "Total reward: -117738\n",
            "Average reward per episode: -11.7738\n",
            "Success rate: 96.74000000000001%\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "weWwEl37hF8i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crBqd_pthJHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experiment Four**"
      ],
      "metadata": {
        "id": "Bc2MU32m2kyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Somewhat similar to Experiment 1c or 2; except use =0.3 and =0.5 in conjunction with either Q-earning or SARSA.\n",
        "*   Run PRANDOM for the first 500 steps; next, you run PEXPLOIT; however, after a terminal state is reached the third time change the 2 pickup locations to: (2, 3, 3) and (1, 3, 1); the drop off locations and the Q-table remain unchanged;\n",
        "*   Finally continue running PEXPLOIT with the new pickup locations until the agent reaches a terminal state the 6th time.\n",
        "*   When interpreting the results of this experiment center on analyzing on how well the learning strategy was able to adapt to the change of the pickup locations and to which extend it was able to learn new paths and unlearn old paths\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KhvL64go2vOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "def choose_action(state, q_table, epsilon):\n",
        "    if np.random.uniform(0, 1) < epsilon:\n",
        "        action = np.random.choice(env.action_space.n)\n",
        "    else:\n",
        "        action = np.argmax(q_table[state])\n",
        "    return action\n",
        "\n",
        "def learn(state, action, reward, next_state, next_action, q_table, alpha, gamma):\n",
        "    predict = q_table[state, action]\n",
        "    target = reward + gamma * q_table[next_state, next_action]\n",
        "    q_table[state, action] += alpha * (target - predict)\n",
        "\n",
        "def run_experiment(env, epsilon, alpha, gamma, num_random_steps, num_exploit_steps):\n",
        "    q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "    # Run PRANDOM policy for num_random_steps\n",
        "    state = env.reset()\n",
        "    for _ in range(num_random_steps):\n",
        "        action = env.action_space.sample()\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        learn(state, action, reward, next_state, choose_action(next_state, q_table, epsilon), q_table, alpha, gamma)\n",
        "        state = next_state if not done else env.reset()\n",
        "\n",
        "    # Run PEXPLOIT policy until terminal state is reached 3 times\n",
        "    terminal_count = 0\n",
        "    state = env.reset()\n",
        "    while terminal_count < 3:\n",
        "        action = choose_action(state, q_table, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        learn(state, action, reward, next_state, choose_action(next_state, q_table, epsilon), q_table, alpha, gamma)\n",
        "        state = next_state if not done else env.reset()\n",
        "        terminal_count += done\n",
        "\n",
        "    # Change pickup locations\n",
        "    env.change_pickup_locations([(2, 3, 3), (1, 3, 1)])\n",
        "\n",
        "    # Run PEXPLOIT policy until terminal state is reached 6 times\n",
        "    terminal_count = 0\n",
        "    state = env.reset()\n",
        "    while terminal_count < 6:\n",
        "        action = choose_action(state, q_table, epsilon)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        learn(state, action, reward, next_state, choose_action(next_state, q_table, epsilon), q_table, alpha, gamma)\n",
        "        state = next_state if not done else env.reset()\n",
        "        terminal_count += done\n",
        "\n",
        "    return q_table\n",
        "\n",
        "# Initialize the environment\n",
        "env = gym.make('Taxi-v3')\n",
        "\n",
        "# Set the experiment parameters\n",
        "epsilon = 0.3\n",
        "alpha = 0.5\n",
        "gamma = 0.99\n",
        "num_random_steps = 500\n",
        "num_exploit_steps = 10000\n",
        "\n",
        "# Run the experiment\n",
        "q_table = run_experiment(env, epsilon, alpha, gamma, num_random_steps, num_exploit_steps)\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ],
      "metadata": {
        "id": "-x2hnetq2oQO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "outputId": "50aff05d-b528-42ca-9da4-0e02b350606a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7173d9bdb163>\u001b[0m in \u001b[0;36m<cell line: 63>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;31m# Run the experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0mq_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_random_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_exploit_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Close the environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7173d9bdb163>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(env, epsilon, alpha, gamma, num_random_steps, num_exploit_steps)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Change pickup locations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_pickup_locations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Run PEXPLOIT policy until terminal state is reached 6 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/gym/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"accessing private attribute '{name}' is prohibited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 'change_pickup_locations'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0ffRxSfk2uMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from collections import defaultdict\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "\n",
        "# Initialize variables\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "n_episodes = 10000\n",
        "max_iter_episode = 100\n",
        "exploration_proba = 1\n",
        "exploration_decreasing_decay = 0.001\n",
        "min_exploration_proba = 0.01\n",
        "gamma = 0.99\n",
        "learning_rates = [0.3, 0.5]\n",
        "\n",
        "# Modify the environment\n",
        "def modify_taxi_environment(env, new_pickup_locations):\n",
        "    env.locs = new_pickup_locations\n",
        "\n",
        "# Initialize new pickup locations\n",
        "new_pickup_locations = [(2, 3, 3), (1, 3, 1)]\n",
        "\n",
        "def choose_action_epsilon_greedy(state, Q_table, exploration_proba):\n",
        "    if np.random.uniform(0, 1) < exploration_proba:\n",
        "        action = env.action_space.sample()\n",
        "    else:\n",
        "        action = np.argmax(Q_table[state, :])\n",
        "    return action\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Experiment with learning rate α={lr}\")\n",
        "    Q_table_q_learning = np.zeros((n_observations, n_actions))\n",
        "    rewards_per_episode_q_learning = []\n",
        "    success_count = 0\n",
        "    terminal_state_count = 0\n",
        "    steps = 0\n",
        "\n",
        "    # Train the agent\n",
        "    for e in range(n_episodes):\n",
        "        if terminal_state_count == 6:\n",
        "            break\n",
        "\n",
        "        # Reset the environment for a new episode\n",
        "        current_state_q_learning = env.reset()\n",
        "        done_q_learning = False\n",
        "        total_episode_reward_q_learning = 0\n",
        "\n",
        "        # Run the episode\n",
        "        for i in range(max_iter_episode):\n",
        "            if terminal_state_count == 6:\n",
        "                break\n",
        "\n",
        "            # Q-Learning\n",
        "            if not done_q_learning:\n",
        "                if steps < 500:\n",
        "                    action_q_learning = env.action_space.sample()\n",
        "                else:\n",
        "                    action_q_learning = choose_action_epsilon_greedy(current_state_q_learning, Q_table_q_learning, exploration_proba)\n",
        "\n",
        "                # Take the chosen action and observe the next state and reward\n",
        "                next_state_q_learning, reward_q_learning, done_q_learning, _ = env.step(action_q_learning)\n",
        "\n",
        "                # Update the Q-table using the Q-learning iteration\n",
        "                Q_table_q_learning[current_state_q_learning, action_q_learning] = (1 - lr) * Q_table_q_learning[current_state_q_learning, action_q_learning] + lr * (reward_q_learning + gamma * np.max(Q_table_q_learning[next_state_q_learning, :]))\n",
        "\n",
        "                # Update the total reward for the episode\n",
        "                total_episode_reward_q_learning += reward_q_learning\n",
        "\n",
        "                # Update the current state\n",
        "                current_state_q_learning = next_state_q_learning\n",
        "\n",
        "            if done_q_learning:\n",
        "                terminal_state_count += 1\n",
        "\n",
        "                if terminal_state_count == 3:\n",
        "                    modify_taxi_environment(env, new_pickup_locations)\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "    print(f\"Total reward: {np.sum(rewards_per_episode_q_learning)}\")\n",
        "    print(f\"Average reward per episode: {np.mean(rewards_per_episode_q_learning)}\")\n",
        "    print(f\"Terminal states reached: {terminal_state_count}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "t-vAiTtRKCsp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7373839-9cb9-4826-e595-2f150e58f014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment with learning rate α=0.3\n",
            "Total reward: 0.0\n",
            "Average reward per episode: nan\n",
            "Terminal states reached: 6\n",
            "\n",
            "\n",
            "Experiment with learning rate α=0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward: 0.0\n",
            "Average reward per episode: nan\n",
            "Terminal states reached: 6\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}